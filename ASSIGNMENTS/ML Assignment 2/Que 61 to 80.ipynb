{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "61 . What is feature scaling, and why is it important in machine learning\n",
    "\n",
    "62 . How does the Naïve Bayes algorithm handle categorical features\n",
    "\n",
    "63 . Explain the concept of prior and posterior probabilities in Naïve Bayes\n",
    "\n",
    "64 . What is Laplace smoothing, and why is it used in Naïve Bayes\n",
    "\n",
    "65 . Can Naïve Bayes handle continuous features\n",
    "\n",
    "67 . What are the assumptions of the Naïve Bayes algorithm\n",
    "\n",
    "68 . How does Naïve Bayes handle missing values\n",
    "\n",
    "69 . What are some common applications of Naïve Bayes\n",
    "\n",
    "70 . Explain the difference between generative and discriminative models\n",
    "\n",
    "71 . How does the decision boundary of a Naïve Bayes classifier look like for binary classification tasks\n",
    "\n",
    "72 . What is the difference between multinomial Naïve Bayes and Gaussian Naïve Bayes\n",
    "\n",
    "73 . How does Naïve Bayes handle numerical instability issues\n",
    "\n",
    "74 . What is the Laplacian correction, and when is it used in Naïve Bayes\n",
    "\n",
    "75 . Can Naïve Bayes be used for regression tasks\n",
    "\n",
    "76 . Explain the concept of conditional independence assumption in Naïve Bayes\n",
    "\n",
    "77 . What are some drawbacks of the Naïve Bayes algorithm\n",
    "\n",
    "78 . Explain the concept of smoothing in Naïve Bayes\n",
    "\n",
    "79 . How does Naïve Bayes handle categorical features with a large number of categories\n",
    "\n",
    "80 . How does Naïve Bayes handle imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Concepts and Naïve Bayes Algorithm\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "**Definition:**\n",
    "Feature scaling is the process of normalizing the range of independent variables or features of data.\n",
    "\n",
    "**Importance:**\n",
    "- Ensures features contribute equally to the result.\n",
    "- Improves the performance of gradient descent-based algorithms.\n",
    "- Reduces the effect of feature dominance.\n",
    "\n",
    "## Naïve Bayes Algorithm (continued)\n",
    "\n",
    "**Prior and Posterior Probabilities:**\n",
    "\n",
    "- **Prior Probability:** Probability of a class before observing the features.\n",
    "  - **Formula:** \\( P(C) \\)\n",
    "  \n",
    "- **Posterior Probability:** Probability of a class given the observed features.\n",
    "  - **Formula:** \\( P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)} \\)\n",
    "\n",
    "**Laplace Smoothing:**\n",
    "\n",
    "**Definition:**\n",
    "Laplace smoothing is a technique to handle zero probabilities in Naïve Bayes by adding a small constant (usually 1) to all counts.\n",
    "\n",
    "**Purpose:**\n",
    "- Prevents zero probabilities and ensures that all feature values have a non-zero probability.\n",
    "\n",
    "**Handling Continuous Features:**\n",
    "\n",
    "Naïve Bayes can handle continuous features by assuming they follow a normal distribution. The Gaussian Naïve Bayes variant is used for this purpose.\n",
    "\n",
    "**Assumptions of Naïve Bayes:**\n",
    "\n",
    "- **Conditional Independence:** Assumes that features are independent given the class label.\n",
    "- **Feature Distribution:** Assumes that feature values are distributed according to a specific distribution (e.g., Gaussian for continuous features).\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "- **Ignore:** Exclude instances with missing values.\n",
    "- **Impute:** Fill missing values with mean, median, or mode.\n",
    "- **Probabilistic Imputation:** Estimate missing values based on probabilities.\n",
    "\n",
    "**Common Applications:**\n",
    "\n",
    "- Spam filtering\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Medical diagnosis\n",
    "\n",
    "**Generative vs. Discriminative Models:**\n",
    "\n",
    "- **Generative Models:** Model the joint probability distribution \\( P(X, Y) \\). Example: Naïve Bayes.\n",
    "- **Discriminative Models:** Model the conditional probability \\( P(Y|X) \\). Example: Logistic Regression, SVM.\n",
    "\n",
    "**Decision Boundary for Binary Classification:**\n",
    "\n",
    "The decision boundary of a Naïve Bayes classifier is typically a linear boundary in the feature space, determined by the posterior probabilities of the classes.\n",
    "\n",
    "**Difference Between Multinomial and Gaussian Naïve Bayes:**\n",
    "\n",
    "- **Multinomial Naïve Bayes:** Used for categorical data and counts occurrences of feature values.\n",
    "- **Gaussian Naïve Bayes:** Used for continuous data and assumes feature values are normally distributed.\n",
    "\n",
    "**Handling Numerical Instability:**\n",
    "\n",
    "Naïve Bayes handles numerical instability issues using logarithms to compute probabilities, which helps to avoid very small numbers that can lead to numerical errors.\n",
    "\n",
    "**Laplacian Correction:**\n",
    "\n",
    "**Definition:**\n",
    "Laplacian correction is a type of Laplace smoothing that adds 1 to each count to ensure non-zero probabilities.\n",
    "\n",
    "**When Used:**\n",
    "- Applied in cases where there are zero counts in categorical data.\n",
    "\n",
    "**Naïve Bayes for Regression:**\n",
    "\n",
    "Naïve Bayes is primarily a classification algorithm and is not typically used for regression tasks. However, variations like Gaussian Naïve Bayes can handle continuous output to some extent.\n",
    "\n",
    "**Conditional Independence Assumption:**\n",
    "\n",
    "**Concept:**\n",
    "Naïve Bayes assumes that all features are conditionally independent given the class label. This simplifies the computation but may not hold true in practice.\n",
    "\n",
    "**Drawbacks of Naïve Bayes:**\n",
    "\n",
    "- **Independence Assumption:** Assumes features are independent, which may not be realistic.\n",
    "- **Poor Performance with Highly Correlated Features:** Can lead to suboptimal performance if features are correlated.\n",
    "- **Limited to Simple Relationships:** Not suitable for complex data structures.\n",
    "\n",
    "**Smoothing in Naïve Bayes:**\n",
    "\n",
    "**Concept:**\n",
    "Smoothing techniques like Laplace smoothing prevent zero probabilities in categorical features and ensure that all possible feature values contribute to the probability calculation.\n",
    "\n",
    "**Handling Categorical Features with Large Categories:**\n",
    "\n",
    "- **Grouping:** Combine similar categories to reduce the number of unique values.\n",
    "- **Feature Hashing:** Map categories to fixed-length vectors.\n",
    "- **Embedding:** Use embedding techniques to represent categories in a continuous space.\n",
    "\n",
    "**Handling Imbalanced Datasets:**\n",
    "\n",
    "Naïve Bayes can handle imbalanced datasets by adjusting class priors or using techniques such as oversampling the minority class or undersampling the majority class.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
