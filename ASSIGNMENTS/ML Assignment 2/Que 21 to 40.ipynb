{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "21. What is the Naïve Bayes algorithm\n",
    "\n",
    "22. Why is it called \"Naïve\" Bayes\n",
    "\n",
    "23. How does Naïve Bayes handle continuous and categorical features\n",
    "\n",
    "24. Explain the concept of prior and posterior probabilities in Naïve Bayes\n",
    "\n",
    "25. What is Laplace smoothing and why is it used in Naïve Bayes\n",
    "\n",
    "26. Can Naïve Bayes be used for regression tasks\n",
    "\n",
    "27. How do you handle missing values in Naïve Bayes\n",
    "\n",
    "28. What are some common applications of Naïve Bayes\n",
    "\n",
    "29. Explain the concept of feature independence assumption in Naïve Bayes.\n",
    "\n",
    "30. How does Naïve Bayes handle categorical features with a large number of categories\n",
    "\n",
    "31. What is the curse of dimensionality, and how does it affect machine learning algorithms\n",
    "\n",
    "32. Explain the bias-variance tradeoff and its implications for machine learning models\n",
    "\n",
    "33. What is cross-validation, and why is it used\n",
    "\n",
    "34. Explain the difference between parametric and non-parametric machine learning algorithms\n",
    "\n",
    "35. What is feature scaling, and why is it important in machine learning\n",
    "\n",
    "36. What is regularization, and why is it used in machine learning\n",
    "\n",
    "37. Explain the concept of ensemble learning and give an example\n",
    "\n",
    "38. What is the difference between bagging and boosting\n",
    "\n",
    "39. What is the difference between a generative model and a discriminative model\n",
    "\n",
    "40. Explain the concept of batch gradient descent and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes and Advanced Machine Learning Concepts\n",
    "\n",
    "## Naïve Bayes Algorithm\n",
    "\n",
    "**Definition:**\n",
    "Naïve Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes independence between the features.\n",
    "\n",
    "## Why is it called \"Naïve\" Bayes\n",
    "\n",
    "**Reason:**\n",
    "The term \"naïve\" comes from the assumption that all features are independent of each other, which is a simplification and rarely true in real-world scenarios.\n",
    "\n",
    "## Handling Continuous and Categorical Features\n",
    "\n",
    "**Continuous Features:**\n",
    "- Naïve Bayes handles continuous features by assuming they follow a normal distribution. The Gaussian Naïve Bayes variant is commonly used.\n",
    "\n",
    "**Categorical Features:**\n",
    "- Categorical features are handled using the multinomial or Bernoulli variants of Naïve Bayes, which count the frequency or presence of features.\n",
    "\n",
    "## Prior and Posterior Probabilities in Naïve Bayes\n",
    "\n",
    "**Prior Probability:**\n",
    "- The probability of a class before observing the features.\n",
    "- **Formula:** \\( P(C) \\)\n",
    "\n",
    "**Posterior Probability:**\n",
    "- The probability of a class after observing the features.\n",
    "- **Formula:** \\( P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)} \\)\n",
    "\n",
    "## Laplace Smoothing\n",
    "\n",
    "**Definition:**\n",
    "Laplace smoothing is a technique to handle zero probabilities in Naïve Bayes by adding a small value (usually 1) to all counts.\n",
    "\n",
    "**Purpose:**\n",
    "- Prevents zero probabilities by ensuring all categories have a non-zero probability.\n",
    "\n",
    "## Naïve Bayes for Regression Tasks\n",
    "\n",
    "**Usage:**\n",
    "Naïve Bayes is typically not used for regression tasks as it is inherently a classification algorithm. However, variations like Gaussian Naïve Bayes can handle continuous output to some extent.\n",
    "\n",
    "## Handling Missing Values in Naïve Bayes\n",
    "\n",
    "**Methods:**\n",
    "- **Ignore:** Exclude instances with missing values.\n",
    "- **Imputation:** Fill missing values with the mean, median, or mode.\n",
    "- **Probabilistic Imputation:** Use probabilities to estimate the missing values.\n",
    "\n",
    "## Common Applications of Naïve Bayes\n",
    "\n",
    "- Spam filtering\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Medical diagnosis\n",
    "\n",
    "## Feature Independence Assumption\n",
    "\n",
    "**Concept:**\n",
    "Naïve Bayes assumes that all features are independent given the class label, which simplifies the computation but may not hold in practice.\n",
    "\n",
    "## Handling Categorical Features with a Large Number of Categories\n",
    "\n",
    "**Methods:**\n",
    "- **Grouping:** Combine similar categories to reduce the number of unique values.\n",
    "- **Feature Hashing:** Map categories to fixed-length vectors.\n",
    "- **Embedding:** Use embedding techniques to represent categories in a continuous space.\n",
    "\n",
    "## Curse of Dimensionality\n",
    "\n",
    "**Definition:**\n",
    "The curse of dimensionality refers to the challenges and issues that arise when analyzing and organizing data in high-dimensional spaces.\n",
    "\n",
    "**Effects:**\n",
    "- Increased sparsity\n",
    "- Higher computational cost\n",
    "- Overfitting\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "**Concept:**\n",
    "The bias-variance tradeoff is the balance between a model's ability to generalize to new data (low variance) and its accuracy on the training data (low bias).\n",
    "\n",
    "**Implications:**\n",
    "- High bias: Underfitting, poor model performance.\n",
    "- High variance: Overfitting, poor generalization.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "**Definition:**\n",
    "Cross-validation is a technique used to evaluate the performance of a model by dividing the data into training and validation sets multiple times.\n",
    "\n",
    "**Purpose:**\n",
    "- To ensure the model generalizes well to unseen data.\n",
    "- To tune hyperparameters effectively.\n",
    "\n",
    "## Parametric vs. Non-Parametric Algorithms\n",
    "\n",
    "**Parametric Algorithms:**\n",
    "- Assumes a fixed number of parameters.\n",
    "- Example: Linear regression, logistic regression.\n",
    "\n",
    "**Non-Parametric Algorithms:**\n",
    "- No fixed number of parameters, can grow with the data.\n",
    "- Example: Decision trees, k-nearest neighbors.\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "**Definition:**\n",
    "Feature scaling is the process of normalizing the range of independent variables.\n",
    "\n",
    "**Importance:**\n",
    "- Ensures features are on a similar scale.\n",
    "- Improves the performance of gradient descent-based algorithms.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "**Definition:**\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty to the model's complexity.\n",
    "\n",
    "**Types:**\n",
    "- **L1 (Lasso) Regularization:** Adds absolute value of coefficients.\n",
    "- **L2 (Ridge) Regularization:** Adds squared value of coefficients.\n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "**Concept:**\n",
    "Ensemble learning combines multiple models to improve overall performance.\n",
    "\n",
    "**Example:**\n",
    "- Random Forest: An ensemble of decision trees.\n",
    "\n",
    "## Bagging vs. Boosting\n",
    "\n",
    "**Bagging:**\n",
    "- Bootstrap Aggregating: Trains multiple models in parallel on different subsets of data.\n",
    "- Example: Random Forest.\n",
    "\n",
    "**Boosting:**\n",
    "- Trains models sequentially, each model correcting the errors of the previous one.\n",
    "- Example: AdaBoost, Gradient Boosting.\n",
    "\n",
    "## Generative vs. Discriminative Models\n",
    "\n",
    "**Generative Models:**\n",
    "- Models the joint probability distribution \\( P(X, Y) \\).\n",
    "- Example: Naïve Bayes, Gaussian Mixture Models.\n",
    "\n",
    "**Discriminative Models:**\n",
    "- Models the conditional probability \\( P(Y|X) \\).\n",
    "- Example: Logistic Regression, SVM.\n",
    "\n",
    "## Batch Gradient Descent vs. Stochastic Gradient Descent\n",
    "\n",
    "**Batch Gradient Descent:**\n",
    "- Uses the entire dataset to compute gradients and update parameters.\n",
    "- Converges smoothly but can be slow for large datasets.\n",
    "\n",
    "**Stochastic Gradient Descent:**\n",
    "- Uses a single data point to compute gradients and update parameters.\n",
    "- Faster but can have high variance in updates.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
