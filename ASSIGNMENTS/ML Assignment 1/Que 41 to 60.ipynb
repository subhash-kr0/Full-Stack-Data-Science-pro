{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 41. Discuss the implications of using data interpolation in machine learning\n",
    " \n",
    " 42. What are outliers in a dataset\n",
    " \n",
    " 43. Explain the impact of outliers on machine learning models\n",
    " \n",
    " 44. Discuss techniques for identifying outliers\n",
    " \n",
    " 45. How can outliers be handled in a dataset\n",
    " \n",
    " 46. Compare and contrast Filter, Wrapper, and Embedded methods for feature selection\n",
    " \n",
    " 47. Provide examples of algorithms associated with each method\n",
    " \n",
    " 48. Discuss the advantages and disadvantages of each feature selection method\n",
    " \n",
    " 49. Explain the concept of feature scaling\n",
    " \n",
    " 50. Describe the process of standardization\n",
    " \n",
    " 51. How does mean normalization differ from standardization\n",
    " \n",
    " 52. What is the purpose of unit vector scaling\n",
    " \n",
    " 53. Discuss the advantages and disadvantages of Min-Max scaling\n",
    " \n",
    " 54. Define Principle Component Analysis (PCA)\n",
    " \n",
    " 55. Explain the steps involved in PCA\n",
    " \n",
    " 56. Discuss the significance of eigenvalues and eigenvectors in PCA\n",
    " \n",
    " 57. How does PCA help in dimensionality reduction\n",
    " \n",
    " 58. Define data encoding and its importance in machine learning\n",
    " \n",
    " 59. Explain Nominal Encoding and provide an example.\n",
    " \n",
    " 60. Discuss the process of One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Interpolation\n",
    "\n",
    "**Implications in Machine Learning:**\n",
    "- **Advantages:**\n",
    "  - Maintains data continuity.\n",
    "  - Improves model training with complete datasets.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Can introduce bias.\n",
    "  - May distort the underlying data distribution.\n",
    "\n",
    "## Outliers in a Dataset\n",
    "\n",
    "**Definition:**\n",
    "Outliers are data points that significantly differ from other observations. They can be unusually high or low values.\n",
    "\n",
    "**Impact on Machine Learning Models:**\n",
    "- **Negative Impact:**\n",
    "  - Skew model training.\n",
    "  - Increase model variance.\n",
    "  - Lead to inaccurate predictions.\n",
    "\n",
    "**Techniques for Identifying Outliers:**\n",
    "- **Statistical Methods:** Z-score, IQR (Interquartile Range).\n",
    "- **Visual Methods:** Box plots, scatter plots.\n",
    "- **Machine Learning Methods:** Isolation Forest, DBSCAN.\n",
    "\n",
    "**Handling Outliers:**\n",
    "- **Removal:** Delete outlier data points.\n",
    "- **Transformation:** Apply transformations like log or square root.\n",
    "- **Imputation:** Replace outliers with mean, median, or mode.\n",
    "- **Robust Models:** Use algorithms less sensitive to outliers (e.g., tree-based methods).\n",
    "\n",
    "## Feature Selection Methods\n",
    "\n",
    "**Filter Methods:**\n",
    "- **Definition:** Select features based on statistical measures.\n",
    "- **Algorithms:** Chi-square test, ANOVA, correlation coefficient.\n",
    "- **Advantages:** Fast, computationally efficient.\n",
    "- **Disadvantages:** Ignore feature interaction.\n",
    "\n",
    "**Wrapper Methods:**\n",
    "- **Definition:** Use a predictive model to evaluate feature subsets.\n",
    "- **Algorithms:** Recursive Feature Elimination (RFE), forward selection, backward elimination.\n",
    "- **Advantages:** Consider feature interaction.\n",
    "- **Disadvantages:** Computationally expensive, risk of overfitting.\n",
    "\n",
    "**Embedded Methods:**\n",
    "- **Definition:** Perform feature selection during model training.\n",
    "- **Algorithms:** LASSO (L1 regularization), Elastic Net, tree-based methods.\n",
    "- **Advantages:** Integrated with model training, efficient.\n",
    "- **Disadvantages:** Model-dependent.\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "**Concept:**\n",
    "Adjusting the scale of features to ensure they contribute equally to model training.\n",
    "\n",
    "**Standardization:**\n",
    "- **Process:** \n",
    "  1. Subtract the mean of each feature.\n",
    "  2. Divide by the standard deviation.\n",
    "- **Formula:** \\( z = \\frac{x - \\mu}{\\sigma} \\)\n",
    "  \n",
    "**Mean Normalization vs. Standardization:**\n",
    "- **Mean Normalization:** Scales data to a [0, 1] range using \\( \\frac{x - \\mu}{x_{max} - x_{min}} \\).\n",
    "- **Standardization:** Centers and scales data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Unit Vector Scaling:**\n",
    "- **Purpose:** Converts data to a unit vector (magnitude of 1).\n",
    "- **Formula:** \\( x' = \\frac{x}{||x||} \\)\n",
    "\n",
    "**Min-Max Scaling:**\n",
    "- **Advantages:**\n",
    "  - Preserves data relationships.\n",
    "  - Suitable for algorithms requiring bounded input.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Sensitive to outliers.\n",
    "  - Does not handle skewed distributions well.\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "**Definition:**\n",
    "A dimensionality reduction technique that transforms data into a new coordinate system, where the greatest variance comes to lie on the first principal component.\n",
    "\n",
    "**Steps Involved:**\n",
    "1. **Standardize Data:** Center and scale the data.\n",
    "2. **Compute Covariance Matrix:** Calculate the covariance between features.\n",
    "3. **Calculate Eigenvalues and Eigenvectors:** Determine the principal components.\n",
    "4. **Sort Eigenvalues and Eigenvectors:** Order by significance.\n",
    "5. **Transform Data:** Project data onto the new principal components.\n",
    "\n",
    "**Significance of Eigenvalues and Eigenvectors:**\n",
    "- **Eigenvalues:** Measure the variance explained by each principal component.\n",
    "- **Eigenvectors:** Define the direction of the principal components.\n",
    "\n",
    "**Dimensionality Reduction:**\n",
    "PCA reduces the number of features while preserving as much variance as possible, simplifying models and reducing computational costs.\n",
    "\n",
    "## Data Encoding\n",
    "\n",
    "**Definition:**\n",
    "Transforming categorical data into numerical format for machine learning models.\n",
    "\n",
    "**Importance:**\n",
    "- Enables models to process categorical data.\n",
    "- Preserves information content.\n",
    "\n",
    "**Nominal Encoding:**\n",
    "- **Definition:** Assigns a unique integer to each category.\n",
    "- **Example:** [\"Red\", \"Blue\", \"Green\"] -> [1, 2, 3]\n",
    "\n",
    "**One Hot Encoding:**\n",
    "- **Process:**\n",
    "  1. Create binary columns for each category.\n",
    "  2. Assign 1 to the corresponding category and 0 to others.\n",
    "- **Example:**\n",
    "  - [\"Red\", \"Blue\", \"Green\"] ->\n",
    "    | Red | Blue | Green |\n",
    "    | --- | ---- | ----- |\n",
    "    | 1   | 0    | 0     |\n",
    "    | 0   | 1    | 0     |\n",
    "    | 0   | 0    | 1     |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
